{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sustainable Software Development, block course, March 2021**  \n",
    "*Scientific Software Center, Institute for Scientific Computing, Dr. Inga Ulusoy*\n",
    "\n",
    "# Analysis of the data\n",
    "\n",
    "Imagine you perform a \"measurement\" of some type and obtain \"scientific data\". You know what your data represents, but you have only a vague idea how different features in the data are connected, and what information you can extract from the data.\n",
    "\n",
    "You would start first with going through the data, making sure your data set is complete and that the result is reasonable. Imagine this already happened.\n",
    "\n",
    "In the next step, you would inspect your data more closely and try to identify structures. That is the step that we are focusing on in this unit.\n",
    "\n",
    "In the `data` folder, you will find several data files (`*.t` and `*.dat`). These are data files generated through some \"new approach\" that hasn't been used in your lab before. No previous analysis software exists, and you are going to establish a protocol for this \"new approach\" and \"publish your results\".\n",
    "\n",
    "The data can be grouped into two categories: \n",
    "1. data to be analyzed using statistical methods;\n",
    "2. data to be analyzed using numerical methods.\n",
    "\n",
    "In your hypothetical lab, you are an \"expert\" in one particular \"method\", and your co-worker is an \"expert\" in the other. Combined these two methods will lead to much more impactful results than if only one of you analyzed the data. Now, the task in this course is to be solved collaboratively with your team member working on one of the analysis approaches, and you working on the other. You will both implement functionality into the same piece of \"software\", but do so collaboratively through git.\n",
    "\n",
    "As you do not know yet which analysis is most meaningful for your data, and how to implement it, you will start with a jupyter notebook. You and your team member will work on the same notebook that will be part of a github repository for your project. This is the task for today. Discuss with your team members who will work on the statistical and who on the numerical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Generate a github repository with the relevant files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Clone the repository to your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Start working on task 1 for your analysis approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Create your own branch of the repository and commit your changes to your branch; push to the remote repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Open a `pull request` so your team member can review your implementation. Likewise, your team member will ask you to review theirs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "\n",
    "Merge the changes in your branch into `main`. Resolve conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7\n",
    "\n",
    "Repeat working on task; committing and pushing to your previously generated branch or a new branch; open a pull request; merge with main; until you have finished all the tasks in your analysis approach. Delete obsolete branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of the analysis notebook\n",
    "\n",
    "**Author : Your Name**  \n",
    "*Date : The date you started working on this*  \n",
    "*Affiliation : The entity under whose name you are working on this*  \n",
    "\n",
    "Place the required modules in the top, followed by required constants and global functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a tuple containing the file names\n",
    "filenames = 'npop.t', 'efield.t', 'expec.t', 'table.dat', 'nstate_i.t'\n",
    "filedir = '../../data/'\n",
    "# maximum number of plots generated with seaborn (only first thresh-1 columns are plotted at most)\n",
    "thresh = 50\n",
    "# threshold for variance so that feature is evaluated as significant\n",
    "threshd = 1.0E-5\n",
    "# threshold for value to be considered different than zero\n",
    "threshv = 1.0E-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical analysis\n",
    "\n",
    "Find correlations in the data sets. Analyse the data statistically and plot your results.  \n",
    "\n",
    "Here we would want to do everything with pandas and leave the data in a dataframe. The files that are relevant to you are `expect.t`, `npop.t` and `table.dat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read in the data files\n",
    "def read_in_df(filename, filedir):\n",
    "    name = '{}{}'.format(filedir,filename)\n",
    "    print('Reading from file {}'.format(name))\n",
    "    temp = pd.read_csv(name, '\\s+')\n",
    "    return temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read in the data files\n",
    "def read_in(filename, filedir):\n",
    "    name = '{}{}'.format(filedir,filename)\n",
    "    print('Reading from file {}'.format(name))\n",
    "    vals = loadtxt(name, skiprows=1)\n",
    "    vals = vals.T\n",
    "    return vals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Read in expect.t and plot relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = read_in_df(filenames[2],filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the dataframe using seaborn\n",
    "sns.relplot(data = df1, kind=\"line\", x=\"time\", y=\"<z>\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df1, corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can discard the entries norm, \\<x>, and \\<y> as these are mostly constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop([\"norm\",\"<x>\",\"<y>\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Create plots of the relevant data and save as .pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Read in file `npop.t` and analyze correlations in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = read_in_df(filenames[1],filedir)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard all columns with variance below a set threshold - we can consider them as constant\n",
    "df2.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(df2.var()[df2.var() < threshd].index.values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the remaining columns. Seaborn prefers \"long format\" (one column for all measurement values, one column to indicate the type) as input, whereas the cvs is in \"wide format\" (one column per measurement type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='time', y='value', hue='variable', \n",
    "             data=pd.melt(df2, ['time']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df2, kind='kde', corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify the pairwise correlation in the data\n",
    "\n",
    "- negative correlation: y values decrease for increasing x - large values of one feature correspond to small values of the other feature\n",
    "- weak or no correlation: no trend observable, association between two features is hardly observable\n",
    "- positive correlation: y values increase for decreasing x - small values of one feature correspond to small values of the other feature\n",
    "\n",
    "Remember that correlation does not indicate causation - the reason that two features are associated can lie in their dependence on same factors.\n",
    "\n",
    "Correlate the value pairs using Pearson's $r$. Pearson's $r$ is a measure of the linear relationship between features:\n",
    "\n",
    "$r = \\frac{\\sum_i(x_i − \\bar{x})(y_i − \\bar{y})}{\\sqrt{\\sum_i(x_i − \\bar{x})^2 \\sum_i(y_i − \\bar{y})^2}}$\n",
    "\n",
    "Here, $\\bar{x}$ and $\\bar{y}$ indicate mean values. $i$ runs over the whole data set. For a positive correlation, $r$ is positive, and negative for a negative correlation, with minimum and maximum values of -1 and 1, indicating a perfectly linear relationship. Weakly or not correlated features are characterized by $r$-values close to 0.\n",
    "\n",
    "Other measures of correlation that can be used are Spearman's rank (value pairs follow monotonic function) or Kendall's $\\tau$ (measures ordinal association), but they do not apply here. You can also define measures yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation Matrix\")\n",
    "print(df2.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that each value is perfectly correlated with itself. We are not interested in the diagonal values and also not in the correlation with time. We also need to get rid of redundant entries. Finally, we need to find the value pairs that exhibit the highest linear correlation. We still want to know if it is positive or negative correlation, so we cannot get rid of the sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_measure(df):\n",
    "    drop_values = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1): # get rid of all diagonal entries and the lower triangular\n",
    "            drop_values.add((cols[i], cols[j]))\n",
    "    return drop_values\n",
    "df2_short = df2.drop([\"time\"], axis = 1) # get rid of time column\n",
    "drop_vals = get_correlation_measure(df2_short) # get rid of time column\n",
    "corr2 = df2_short.corr().unstack()\n",
    "corr2 = corr2.drop(labels=drop_vals).sort_values(ascending=False, key=lambda col: col.abs()) # sort by absolute values but keep sign\n",
    "print(corr2[0:12])                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the entries in the left column are not repeated if they do not change from the row above (so the fourth feature pair is MO3 and MO6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Generate graphs of the relevant data and save as .pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Calculate the Euclidean distance (L2 norm) for the vectors in `table.dat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = read_in(filenames[3],filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need for the first two columns, and replace the NaNs by zero\n",
    "table_vals = delete(values,[0,1],axis=0)\n",
    "table_vals = nan_to_num(table_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate how different the vectors in column 1 are from column 0, column 3 from column 2, and column 5 from column 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_euc_dist(list_ref,list_comp,array_in):\n",
    "    distances = zeros(len(list_ref))\n",
    "    for i in range(len(list_ref)):\n",
    "        distances[i] = linalg.norm(array_in[list_comp[i]]-array_in[list_ref[i]])\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dist = calc_euc_dist([0,2,4],[1,3,5],table_vals)\n",
    "print(out_dist)\n",
    "x = range(0,len(out_dist))\n",
    "plt.bar(x,out_dist)\n",
    "plt.xticks(x, ('x', 'y', 'z'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical analysis\n",
    "\n",
    "Analyze the data using autocorrelation functions and discrete Fourier transforms. Plot your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the data\n",
    "def plot_vals(values,thresh):\n",
    "    if len(values) <= thresh:\n",
    "        maxcol = len(values)\n",
    "    else:\n",
    "        maxcol = thresh\n",
    "    for i in range(1,maxcol):\n",
    "        plt.plot(values[0],values[i])\n",
    "        plt.title('column {}'.format(i))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Read in `efield.t` and Fourier-transform relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in first file\n",
    "values = read_in(filenames[1],filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first set of values\n",
    "plot_vals(values,thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are interested in column 2 since the others are constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the FT - see https://numpy.org/doc/stable/reference/routines.fft.html\n",
    "def do_fft(data,tmax):\n",
    "    data_s = fft.rfft(data)\n",
    "    data_w = fft.rfftfreq(tmax)\n",
    "    # only take the positive frequency components through rfft\n",
    "    return data_w, data_s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which columns are important\n",
    "myvar = var(values, axis=1)\n",
    "print(myvar)\n",
    "# delete those below threshold\n",
    "type(myvar)\n",
    "values = values[nonzero(myvar > threshd)]\n",
    "print(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om, sig = do_fft(values[1],len(values[0]))\n",
    "# at this point not sure why the shift is required\n",
    "# I remember having issues with that previously\n",
    "# could be the time step is too large\n",
    "delta = (om[1] - om[0])/2\n",
    "om = om + delta\n",
    "plt.plot(om, real(sig), label = 'real part')\n",
    "plt.plot(om, imag(sig), label = 'imaginary part')\n",
    "plt.plot(om, abs(sig)**2, label = 'power spectrum')\n",
    "plt.axvline(0.116, 0, 1, color = 'black', label = \"carrier frequency\")\n",
    "plt.xlabel('energy (Eh)')\n",
    "plt.ylabel('intensity (arb. u.)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Generate a plot of your results to be saved as pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Calculate the autocorrelation function from nstate_i.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavef = read_in(filenames[4],filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the time column in a vector and drop from array\n",
    "time = wavef[0]\n",
    "wavef = delete(wavef,[0],axis=0)\n",
    "print(wavef[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to complex array\n",
    "realpart = wavef[0::2]\n",
    "imagpart = wavef[1::2]\n",
    "wavefc = realpart + 1j*imagpart\n",
    "print(wavefc[0])\n",
    "print(realpart[0])\n",
    "print(imagpart[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now construct overlap between first vector and all others\n",
    "def calc_auto(wavef):\n",
    "    aucofu = zeros(len(wavef[0]),dtype = complex)\n",
    "    for i in range(0,len(wavef[0])):\n",
    "        aucofu[i] = sum(wavef[:,0]*conjugate(wavef[:,i]))\n",
    "    return aucofu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucofu = calc_auto(wavefc)\n",
    "print(aucofu)\n",
    "plt.plot(abs(aucofu**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Generate a plot of your results to be saved as pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Discrete Fourier transform of the autocorrelation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the FT\n",
    "# do the FT - see https://numpy.org/doc/stable/reference/routines.fft.html\n",
    "def do_fft(data,tmax):\n",
    "    data_s = fft.fft(data)\n",
    "    data_w = fft.fftfreq(tmax)\n",
    "    # only take the positive frequency components\n",
    "    return data_w[0:tmax//2], data_s[0:tmax//2] \n",
    "\n",
    "energy, spec = do_fft(aucofu,len(time))\n",
    "print(energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(energy,real(spec))\n",
    "#plt.plot(energy,imag(spec))\n",
    "#plt.plot(energy,abs(spec))\n",
    "plt.plot(energy,abs(spec)**2)\n",
    "plt.ylim(-0.1,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Generate a plot of your results to be saved as pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
